# Data Collection Methods

## Overview

This document outlines data collection methods for HERITRACE user testing, designed to capture usability, effectiveness, and user experience across all testing scenarios.

## Data Collection Framework

**Multi-Method Approach**: Quantitative (task completion, timing, interaction data), Qualitative (think-aloud protocols, observations, interviews), Mixed Methods (video analysis, comparative studies)

## Quantitative Data Collection

### Task Completion Metrics
**Completion Rates**: Overall and task-specific completion rates, partial completion classification, abandonment points

**Recording Template**:
```
Task ID: [Scenario-Task-Number]
Participant ID: [Anonymous ID]
Start/End Time: [Timestamps]
Completion Status: [Complete/Partial/Incomplete]
Quality Rating: [1-5 scale]
Notes: [Brief description]
```

**Timing Measurements**: Overall scenario completion time, individual task segments, error recovery time, help consultation time

**Error Classification**: Navigation errors, input errors, conceptual errors, workflow errors

### System Interaction Data
**Interface Usage**: Click/tap frequency and locations, navigation patterns, feature discovery rates, help system usage

**Search Behaviors**: Query construction and refinement, result evaluation patterns, filter and sorting usage, success rates

## Qualitative Data Collection

### Think-Aloud Protocol
**Instructions**: "Please speak aloud about what you're thinking as you work. Tell us about your decisions, confusions, and reactions."

**Facilitator Prompts**: "What are you thinking about right now?", "What would you expect to happen?", "How does this compare to tools you normally use?"

**Recording and Analysis**: High-quality audio capture, real-time annotation of emotional tone and confidence, significant verbalizations

### Observational Data
**Behavioral Categories**: Positive (smooth completion, confident interactions), Neutral (standard progression), Negative (hesitation, confusion, frustration)

**Real-Time Log Format**:
```
Time: [Timestamp]
Participant: [ID]
Behavior: [Description]
Context: [Current task/interface]
Significance: [High/Medium/Low]
```

### Post-Testing Interviews
**Interview Structure** (45 minutes): Overall Experience (10 min), Specific Feature Feedback (15 min), Workflow Integration (10 min), Comparison and Context (10 min)

**Key Question Areas**: Overall experience rating, most/least useful features, workflow integration potential, training needs assessment

### Survey Instruments
**System Usability Scale (SUS)**: [Standard 10-question survey](questionnaires/sus_questionnaire.md)

**Custom Satisfaction Survey** (1-7 scale):
1. Metadata creation process is intuitive
2. Search functionality meets professional needs
3. Interface design supports efficient work
4. System would improve workflow efficiency
5. Feel confident using system for work
6. System handles complex relationships well
7. Change tracking features provide valuable information
8. Would recommend system to colleagues

## Recording and Analysis

### Technical Setup
**Hardware Requirements**: High-resolution screen recording (minimum 1080p), clear audio recording, optional webcam, backup recording systems

**Recording Guidelines**: Full session recording, include breaks and informal discussions, explicit consent, option to pause recording

### Video Analysis
**Analysis Categories**: Interface interaction patterns, problem-solving behaviors, error recovery strategies, feature discovery methods

**Coding Framework**: Timestamp-based event logging, behavioral category classification, significance rating, cross-reference with audio transcripts

## Data Integration

### Multi-Source Synthesis
**Triangulation Methods**: Combine quantitative metrics with qualitative insights, cross-reference video observations with interview responses, validate findings across multiple sources

**Analysis Levels**: Participant-Level (individual user profiles, learning curves), System-Level (overall usability trends, feature effectiveness)

### Quality Assurance
**Validation Measures**: Multiple observer validation, participant verification of interpretation accuracy, cross-checking quantitative measurements, systematic review of data completeness

**Reliability Checks**: Inter-rater reliability for observational data, internal consistency of survey responses, systematic bias identification

## Ethical Considerations

### Privacy Protection
**Data Anonymization**: Participant ID assignment, removal of personally identifiable information, secure storage with limited access, clear data retention timeline

**Informed Consent**: Clear explanation of data collection methods, specific consent for recording, right to withdraw, future use limitations

## Deliverables

### Quantitative Reports
- Task completion and timing statistics
- Error frequency and pattern analysis
- System usability scale results
- Task completion summaries

### Qualitative Analysis
- Thematic analysis of interview responses
- Observational pattern identification
- User journey documentation
- Feature effectiveness assessments

### Integrated Findings
- Comprehensive usability assessment
- User experience insights and recommendations
- System improvement priorities
- Implementation guidance for institutions 