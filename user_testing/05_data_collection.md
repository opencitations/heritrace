# Data Collection Methods for HERITRACE User Testing

## Overview

This document outlines comprehensive data collection methods for HERITRACE user testing, covering both quantitative metrics and qualitative insights. The methodology is designed to capture usability, effectiveness, and user experience data across all testing scenarios.

## Data Collection Framework

### Multi-Method Approach

**Quantitative Data:**
- Task performance metrics
- System interaction data
- Time-based measurements
- Error and completion rates

**Qualitative Data:**
- Think-aloud protocols
- Observational notes
- Post-test interviews
- Satisfaction surveys

**Mixed Methods:**
- Video/screen recording analysis
- Comparative workflow studies
- Pre/post assessment changes

## Quantitative Data Collection

### Task Performance Metrics

#### Completion Rates
**Primary Metrics:**
- Overall task completion rate (percentage)
- Task-specific completion rates
- Partial completion classification
- Abandonment points and reasons

**Data Collection Method:**
- Real-time observation and logging
- Automated system logging where possible
- Post-task verification with participants
- Classification of completion quality (full/partial/incomplete)

**Recording Template:**
```
Task ID: [Scenario-Task-Number]
Participant ID: [Anonymous ID]
Start Time: [Timestamp]
End Time: [Timestamp]
Completion Status: [Complete/Partial/Incomplete]
Quality Rating: [1-5 scale]
Notes: [Brief completion description]
```

#### Time-to-Completion Measurements
**Timing Categories:**
- Overall scenario completion time
- Individual task completion time
- Time to first successful action
- Time spent on error recovery
- Time consulting help/documentation

**Measurement Approach:**
- Automated timestamping for major interactions
- Manual timing for complex tasks
- Segmented timing for multi-step processes
- Comparison against baseline expectations

#### Error Frequency and Types
**Error Classifications:**
- **Navigation Errors**: Wrong interface sections, lost navigation
- **Input Errors**: Incorrect data entry, format mistakes
- **Conceptual Errors**: Misunderstanding of system concepts
- **Workflow Errors**: Incorrect task sequence or approach

**Error Tracking:**
- Real-time error logging during sessions
- Classification of error severity (critical/moderate/minor)
- Recovery time and method documentation
- Pattern identification across participants

### System Interaction Data

#### Interface Usage Patterns
**Interaction Metrics:**
- Click/tap frequency and locations
- Scroll patterns and page engagement
- Feature discovery and adoption rates
- Help system usage frequency

**Recording Methods:**
- Screen recording with interaction overlays
- Browser-based analytics (if applicable)
- Manual observation logs
- Post-session feature usage review

#### Search and Discovery Behaviors
**Search Metrics:**
- Query construction approaches
- Search refinement patterns
- Result evaluation behaviors
- Filter and sorting usage

**Data Points:**
- Number of search attempts per task
- Query length and complexity
- Time spent reviewing results
- Success rate for finding relevant items

### Performance Perception Data

#### System Responsiveness Assessment
**Measurement Areas:**
- Perceived vs. actual response times
- Frustration points due to performance
- Satisfaction with system speed
- Comparison to current tools

**Collection Methods:**
- Post-task responsiveness ratings (1-7 scale)
- Think-aloud comments about performance
- Observation of waiting behaviors
- Direct questions about speed satisfaction

## Qualitative Data Collection

### Think-Aloud Protocol Implementation

#### Protocol Guidelines
**Instructions to Participants:**
"Please speak aloud about what you're thinking as you work. Tell us about your decisions, confusions, and reactions. There are no wrong thoughts - we want to understand your experience."

**Facilitator Prompts:**
- "What are you thinking about right now?"
- "What would you expect to happen if you clicked that?"
- "How does this compare to tools you normally use?"
- "What information are you looking for?"

#### Recording and Analysis
**Audio Recording:**
- High-quality audio capture throughout sessions
- Transcription for detailed analysis
- Timestamp alignment with screen recordings
- Annotation of emotional tone and confidence levels

**Real-Time Notes:**
- Significant verbalizations and insights
- Non-verbal reactions and behaviors
- Moments of confusion or clarity
- Breakthrough understanding moments

### Observational Data Collection

#### Behavioral Observation Framework
**Focus Areas:**
- Problem-solving approaches and strategies
- Confidence levels and uncertainty indicators
- Frustration and satisfaction expressions
- Learning and adaptation patterns

**Observation Categories:**
- **Positive Behaviors**: Smooth task completion, confident interactions, effective problem-solving
- **Neutral Behaviors**: Standard task progression, methodical approaches, routine interactions
- **Negative Behaviors**: Hesitation, confusion, error-prone interactions, frustration indicators

#### Documentation Methods
**Real-Time Observation Log:**
```
Time: [Timestamp]
Participant: [ID]
Behavior: [Description]
Context: [Current task/interface]
Significance: [High/Medium/Low]
Follow-up: [Questions for post-test]
```

**Body Language and Affect:**
- Facial expressions and emotional reactions
- Posture and engagement indicators
- Mouse/keyboard interaction patterns
- Voice tone and speech patterns

### Post-Testing Interviews

#### Structured Interview Protocol

**Interview Sections:**
1. **Overall Experience** (10 minutes)
2. **Specific Feature Feedback** (15 minutes)
3. **Workflow Integration** (10 minutes)
4. **Comparison and Context** (10 minutes)
5. **Recommendations and Priorities** (10 minutes)

#### Interview Questions Framework

**Overall Experience Questions:**
- "How would you describe your overall experience with HERITRACE?"
- "What was your first impression when you started using the system?"
- "What worked better than you expected? What was more difficult?"
- "If you had to explain this tool to a colleague, how would you describe it?"

**Feature-Specific Questions:**
- "Which features did you find most/least useful?"
- "Were there features you expected to find but couldn't locate?"
- "How well did the search functionality meet your needs?"
- "What did you think of the metadata editing interface?"

**Workflow Integration Questions:**
- "How would this tool fit into your current workflow?"
- "What would need to change in your organization to use this effectively?"
- "How does this compare to the tools you currently use for similar tasks?"
- "What barriers do you see to adopting this system?"

**Recommendation Questions:**
- "What improvements would make the biggest difference for your work?"
- "What features are essential vs. nice-to-have?"
- "Would you recommend this tool to colleagues? Why or why not?"
- "What training or support would you need to use this effectively?"

#### Interview Recording and Analysis
**Recording Methods:**
- Audio recording with participant consent
- Detailed written notes during interview
- Post-interview summary and key insights
- Integration with observational data

### Survey Instruments

#### System Usability Scale (SUS)
**Standard SUS Implementation:**
- 10-question standard SUS survey
- 1-5 scale responses
- Post-completion administration
- Benchmark comparison data

#### Custom Satisfaction Surveys

**HERITRACE-Specific Questions:**
```
1. The metadata creation process is intuitive (1-7 scale)
2. The search functionality meets my professional needs (1-7 scale)
3. The interface design supports efficient work (1-7 scale)
4. The system would improve my workflow efficiency (1-7 scale)
5. I feel confident using this system for my work (1-7 scale)
6. The system handles complex bibliographic relationships well (1-7 scale)
7. The change tracking features provide valuable information (1-7 scale)
8. I would recommend this system to colleagues (1-7 scale)
```

**Professional Context Questions:**
- Institution type and size
- Current tools and systems used
- Experience level with similar systems
- Specific professional responsibilities
- Technology comfort level

## Video and Screen Recording

### Recording Setup and Protocol

#### Technical Setup
**Hardware Requirements:**
- High-resolution screen recording (minimum 1080p)
- Clear audio recording for think-aloud protocols
- Optional webcam for facial expressions and gestures
- Backup recording systems for reliability

**Software Configuration:**
- Screen recording with mouse/keyboard highlighting
- Synchronized audio and video streams
- Minimal performance impact on test system
- Easy file management and storage

#### Recording Guidelines
**Session Recording:**
- Full screen recording from session start to finish
- Include breaks and informal discussions
- Capture pre-task setup and post-task discussions
- Ensure participant comfort with recording

**Privacy and Consent:**
- Clear explanation of recording purposes
- Explicit consent for all recording methods
- Option to pause recording if needed
- Secure storage and limited access protocols

### Video Analysis Framework

#### Analysis Categories
**Interface Interaction Patterns:**
- Mouse movement and click patterns
- Keyboard input behaviors
- Scroll and navigation sequences
- Feature discovery methods

**Problem-Solving Behaviors:**
- Approach to new or unfamiliar interfaces
- Error recovery strategies
- Help-seeking behaviors
- Trial-and-error vs. systematic approaches

#### Coding and Analysis Protocol
**Video Coding Framework:**
- Timestamp-based event logging
- Behavioral category classification
- Significance and impact rating
- Cross-reference with audio transcripts

**Analysis Software:**
- Video analysis software for detailed coding
- Statistical analysis of interaction patterns
- Heat mapping of interface usage
- Timeline analysis of user journeys

## Data Integration and Analysis

### Multi-Source Data Synthesis

#### Data Triangulation
**Integration Methods:**
- Combine quantitative metrics with qualitative insights
- Cross-reference video observations with interview responses
- Validate findings across multiple data sources
- Identify consistent patterns and outliers

#### Analysis Framework
**Participant-Level Analysis:**
- Individual performance profiles
- Learning curve documentation
- Personal workflow adaptation patterns
- Specific needs and preferences

**System-Level Analysis:**
- Overall usability metrics and trends
- Feature effectiveness across participants
- Common issues and success patterns
- Interface design impact assessment

### Quality Assurance

#### Data Validation
**Accuracy Checks:**
- Multiple observer validation for subjective assessments
- Participant verification of interpretation accuracy
- Cross-checking quantitative measurements
- Systematic review of data completeness

**Reliability Measures:**
- Inter-rater reliability for observational data
- Test-retest consistency where applicable
- Internal consistency of survey responses
- Systematic bias identification and mitigation

## Ethical Considerations

### Participant Privacy Protection

#### Data De-identification
**Anonymization Procedures:**
- Participant ID assignment and mapping
- Removal of personally identifiable information
- Institutional affiliation generalization
- Secure storage of identifying information

#### Consent and Transparency
**Informed Consent:**
- Clear explanation of data collection methods
- Specific consent for recording and observation
- Right to withdraw or pause participation
- Future use and sharing limitations

### Data Security and Storage

#### Secure Data Management
**Storage Protocols:**
- Encrypted storage for all recorded data
- Limited access with authentication requirements
- Regular backup and integrity verification
- Defined retention and deletion timelines

**Data Sharing Guidelines:**
- De-identified data sharing protocols
- Participant consent for research publication
- Institutional review board compliance
- Professional ethics adherence

## Expected Deliverables

### Data Collection Outputs

**Quantitative Reports:**
- Task completion and timing statistics
- Error frequency and pattern analysis
- System usability scale results
- Performance metric summaries

**Qualitative Analysis:**
- Thematic analysis of interview responses
- Observational pattern identification
- User journey and workflow documentation
- Feature effectiveness assessments

**Integrated Findings:**
- Comprehensive usability assessment
- User experience insights and recommendations
- System improvement priorities
- Implementation guidance for institutions

### Actionable Recommendations

**Design Improvements:**
- Interface design modification recommendations
- Workflow optimization suggestions
- Feature enhancement priorities
- Accessibility improvement identification

**Implementation Support:**
- User training program specifications
- Documentation improvement requirements
- Technical support anticipatory guidance
- Institutional adoption strategy recommendations 