# End User Testing Protocol

## Overview

This protocol evaluates HERITRACE's usability, effectiveness, and user experience for academic professionals who create and manage bibliographic metadata.

## Objectives

**Primary**: Evaluate ease of use for metadata creation/editing, assess workflow integration, test core functionality, evaluate learning curve, assess visual design and navigation

**Secondary**: Identify common user errors and interface issues, evaluate accessibility compliance, test change tracking and provenance features

## Pre-Testing Setup

### Environment Requirements
- Pre-configured HERITRACE instance with sample data
- Modern web browser (Chrome, Firefox, Safari, Edge)
- Stable internet connection
- Audio recording capability for think-aloud protocols

### Sample Data Provided
- 100 sample bibliographic records (books, articles, papers, theses)
- Various completion states (complete, incomplete, errors)
- Complex relationships examples (series, collections, authorship)
- Duplicate entities for merge testing
- Conflicting information for resolution testing

## Testing Sessions

**Note**: Generic tasks below should be replaced with realistic scenarios from [User Scenarios](04_user_scenarios.md) matching participant experience level.

### Session 1: Initial Exploration (1-2 hours)

#### Task 1.1: First Impressions (20 minutes)
**Objective**: Gather initial reactions and assess intuitive understanding
**Tasks**:
1. Log into system
2. Explore interface without specific goals
3. Understand system purpose
4. Identify main navigation elements

#### Task 1.2: Record Examination (25 minutes)
**Objective**: Assess metadata display and organization comprehension
**Tasks**:
1. Open complex bibliographic record
2. Examine metadata fields and structure
3. Understand entity relationships
4. Identify editing organization

#### Task 1.3: Basic Navigation (15 minutes)
**Objective**: Test fundamental interface usability
**Tasks**:
1. Navigate between system sections
2. Use breadcrumb navigation
3. Access help features
4. Test responsive design elements

### Session 2: Core Functionality (1.5-2 hours)

#### Task 2.1: Creating New Record (45 minutes)
**Objective**: Test metadata creation workflow
**Materials**: Complete citation details, author information, publication details
**Tasks**:
1. Initiate new record creation
2. Select appropriate resource type
3. Enter all provided metadata
4. Establish entity relationships
5. Validate and save record

#### Task 2.2: Editing Existing Record (30 minutes)
**Objective**: Test metadata editing workflows
**Setup**: Record with errors and missing information
**Tasks**:
1. Locate record requiring updates
2. Identify errors and missing information
3. Edit metadata fields with corrections
4. Add missing information
5. Save and verify changes

#### Task 2.3: Managing Relationships (30 minutes)
**Objective**: Test complex relationship management
**Tasks**:
1. Identify resources that should be related
2. Establish hierarchical relationships
3. Create cross-references between works
4. Verify relationship displays
5. Navigate through relationships

#### Task 2.4: Entity Merging (30 minutes)
**Objective**: Test duplicate resolution capabilities
**Setup**: Duplicate entities with conflicting information
**Tasks**:
1. Identify duplicate entities
2. Initiate merge process
3. Resolve metadata conflicts
4. Choose/combine information sources
5. Complete merge and verify results

#### Task 2.5: Quality Control (15 minutes)
**Objective**: Test data quality features
**Tasks**:
1. Access validation features
2. Review records for completeness
3. Address validation warnings
4. Understand quality indicators
5. Finalize records

### Session 3: Advanced Features (1-1.5 hours)

#### Task 3.1: Change Tracking (20 minutes)
**Objective**: Test provenance and change tracking
**Tasks**:
1. Access change history for modified records
2. Review provenance information
3. Compare different record versions
4. Understand change sequence
5. Restore previous version if needed

## Post-Testing Interview

### Experience Assessment
1. **Overall Satisfaction**: Experience rating, most liked/frustrating aspects, comparison to current tools
2. **Workflow Integration**: Fit into research workflow, task support, academic activity integration
3. **Learning and Training**: Proficiency time estimate, training needs, most difficult aspects
4. **Feature Priorities**: Most/least valuable features, missing important features


### Usability Questions (1-7 scale)
**Navigation and Interface**: System navigation intuitive, Information well-organized, Visual design supports work, Error messages helpful

**Functionality**: Metadata workflows efficient, Relationship management intuitive, Entity merging effective, Quality control helpful, Change tracking valuable

**Professional Workflow**: System would improve work quality, System would improve efficiency, Would recommend to colleagues, System addresses real problems

## Data Collection

### Quantitative Metrics
- Task completion rates and success levels
- Time to complete major tasks
- Error frequency and failed attempts
- Help/documentation access frequency
- Feature usage patterns
- Navigation patterns and search queries

### Qualitative Data
- Think-aloud protocol insights
- Problem-solving approaches
- Emotional reactions and frustration points
- Confidence levels and uncertainty
- Interview responses and feedback
- Workflow integration insights

### Recording
- Full screen recording throughout sessions
- High-quality audio for think-aloud protocols
- Optional webcam recording for reactions
- Focus on interaction patterns, problem-solving, error recovery

## Success Criteria

### Task Completion
- **Minimum**: 70% completion rate (varying experience levels), 75% basic tasks, 65% creation/editing, 55% advanced features
- **Target**: 85% overall completion, 90% fundamental workflows, 80% creation/editing/merging, 70% collaborative features
- **Optimal**: 95% core functionality, 85% all features, expected completion times, high confidence (>7/10)

### User Satisfaction
- **SUS Scores**: Target 70+, Optimal 80+, Minimum individual 60+
- **Custom Measures**: Overall satisfaction 7+, workflow integration 6+, feature usefulness 7+, 70%+ recommendation rate

### Quality Indicators
- Intuitive navigation without training
- Self-explanatory feature functionality
- Effective error prevention and recovery
- Accessible design for diverse users
- Realistic task completion in reasonable timeframes
- Quality metadata creation capabilities
